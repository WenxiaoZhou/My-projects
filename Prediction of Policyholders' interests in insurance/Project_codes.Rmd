---
title: "Project-MATH5637"
author: "Wenxiao Zhou"
date: "11/6/2020"
output: word_document
---

**Project of MATH-5637**

<Kind remainder: there are some parts of the codes that cost a long time for running, please pay some patience.>

**Introduction**
This project is discussing a problem of prediction of Policyholders' Interests on Cross-Sell Product--Vehicle Insurance. The data set is "train.csv" with 12 variables.

**Exploratory Data Analysis**

```{r}
library(ggplot2)
library(plyr)
library(MASS)
setwd("/Users/zhouwenxiao/Desktop/Project of Math5637")
data<-read.csv("whole.csv")
names(data)<-c("ID","Gender","Age","Drive","Code","Insured","Vage","Damage","Premium","Channel","Vintage","y")
data$logPremium<-log(data$Premium)
data$Gender<-ifelse(data$Gender=="Male",1,0)
data$Vage<-ifelse(data$Vage=="< 1 Year",1,ifelse(data$Vage=="1-2 Year",2,3))
data$Damage<-ifelse(data$Damage=="Yes",1,0)
```
Notes that we use Gender(female,Male) Drive(1,0) Code(unique code) Insured(1,0) Vage(<1,1-2,2) Damage(Yes,No) Channel(variety) as categorical variables, Age Premium Vintage as continuous variables, response variable is y=1 or 0.

Perform some descriptive analysis to the data:

Summary the statistics 
```{r}
summary(data.frame(data$Age,data$logPremium,data$Vintage))
```
Draw the histograms with density curve for Age for response:
```{r}
par(mfrow=c(1,2))
hist(data$Age[which(data$y==1)],freq=FALSE,col="#FFFFCC",xlab="range",main="age historgram of response=1")
rug(jitter(data$Age[which(data$y==1)]))
lines(density(data$Age),col="blue",lwd=2)
hist(data$Age[which(data$y==0)],freq=FALSE,col="#FFFFCC",xlab="range",main="age historgram of response=0")
rug(jitter(data$Age[which(data$y==0)]))
lines(density(data$Age),col="blue",lwd=2)
```

```{r}
#Detect the correlation between the predictor variables and response
library(magrittr)
library(dplyr)
library(corrplot)
data_corr<-data %>% mutate_if(is.factor,as.numeric)
cor(data_corr,use="pairwise.complete.obs",method="spearman") %>%
  corrplot(method="pie",outline=T,addgrid.col="darkgray",mar=c(0.1,0,0.1,0),type="full",rect.col="black",rect.lwd=5,cl.pos="b",tl.col="black",tl.cex=1,cl.cex=0.5,tl.srt=50,col=c('#9999FF','gray','#99CCFFFF'),number.cex=7/ncol(data_corr),main="Correlation Matrix among variables")
```

Detect the distribution of Y:
```{r}
#response y=1/0
table1<-with(data,table(y))
df1<-as.data.frame(table1)
ggplot(data=df1,mapping=aes(x=y,y=Freq))+geom_bar(stat="identity",width=0.5,color='white',fill='#99CCFF')+
  geom_text(aes(label=Freq),vjust=1.6,color="white",size=3.5)+
  labs(x="Interested in Vehicle Insurance or not",y="Frequency",title="Barplot of reponse")+
  theme_minimal()
```

From the Barplot above, we detect there is an imbalanced problem for response. Consider resampling method: use a random sample subset to verify the effectiveness of the model:
(https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)
(https://blog.csdn.net/wong2016/article/details/73913425)

**Method 1: use SMOTE(super-sampling rare events)**

(Reference:https://blog.csdn.net/c1z2w3456789/article/details/80664872
https://blog.csdn.net/jiabiao1602/article/details/42392377)

```{r}
library(DMwR)
library(caret)
library(caTools)
library(plyr)
library(grid)
#library(sampling)
data$y<-factor(data$y)
set.seed(1234)
splitindex<-createDataPartition(data$y,time=1,p=0.8,list=FALSE)
trainsplit<-data[splitindex,]
test<-data[-splitindex,]
prop.table(table(trainsplit$y))
prop.table(table(test$y))
#two categories are balanced
#use SMOTE to deal with unbalanced with each trainsplit and testsplit

#假设初始数据集中有N个少数样本和M个多数的样本，perc.over=a,perc.under=b。首先增加少数派样本的数量，平均每个样本增加a/100个新样本，一共新增了aN/100个全新的少数派样本，并把最初的少数派样本和新增的少数派样本都放入新的数据集中。然后对多数派的样本进行采样，采样数量为(b/100) * aN/100，得到新的多数派样本，将新的多数派样本放入到新的数据集中，这样新的数据集中，少数派样本有(1+a/100)N个,多数派样本有(b/100) * aN/100个 perc.over 不能为0
train<-SMOTE(y~.,trainsplit,perc.over=100,perc.under=200)
train$y<-ifelse(train$y==1,0,1)
train$y<-as.numeric(train$y)
table(train$y)
```
Since the SMOTE function helps simulate some new samples in the minority group, for categorical variables, there exist some categories that are not integers, we should consider these values as errors and delete them.

```{r}
#focus on the categorical variables
table(train$Gender)
table(train$Drive)
table(train$Insured)
table(train$Vage)
table(train$Damage)

#delete
dg<-which(train$Gender!=0&train$Gender!=1)
dd<-which(train$Drive!=0&train$Drive!=1)
di<-which(train$Insured!=0&train$Insured!=1)
tr<-train[-di,]
dv<-which(tr$Vage!=1&tr$Vage!=2&tr$Vage!=3)
tr1<-tr[-dv,]
dd<-which(tr1$Damage!=1&tr1$Damage!=0)
train<-tr1
```


Detecting the relationship between continuous predictors, we draw the correlation plot below. Since correlation coefficients are not too large, preserve all continuous variables.
```{r}
attach(train)
conti<-data.frame(Age,logPremium,Vintage)
cor(conti)
```

For categorical variables, we detect the distributions of Gender(1,0), Drive(1,0):
```{r}
train$y<-as.numeric(train$y)
gender1y<-train$y[which(train$Gender==1)]
count11<-round(sum(gender1y)/length(gender1y)*100,2)
count12<-100-count11
gender0y<-train$y[which(train$Gender==0)]
count01<-round(sum(gender0y)/length(gender0y)*100,2)
count02<-100-count01
x=rep(c('Male','Female'),each=2)
y=rep(c('Interested','Not-interest'),times=2)
z<-c(count11,count12,count01,count02)
df<-data.frame(x=x,y=y,z=z)
ggplot(data=df,mapping=aes(x=factor(x),y=z,fill=y))+
  geom_bar(stat='identity',width=0.4,position='stack')+
  geom_text(mapping=aes(label=z),size=4,vjust=3.5,hjust=0.5,position=position_stack())+
  theme(legend.position="top")+xlab('Gender Category')+
  ylab('Response percentage(%)')+
  ggtitle('Response percentage comparison among gender groups')

drive1y<-train$y[which(train$Drive==1)]
countd11<-round(sum(drive1y)/length(drive1y)*100,2)
countd12<-100-countd11
drive0y<-train$y[which(train$Drive==0)]
countd01<-round(sum(drive0y)/length(drive0y)*100,2)
countd02<-100-countd01
x=rep(c('Have License','No License'),each=2)
y=rep(c('Interested','Not-interest'),times=2)
z<-c(countd11,countd12,countd01,countd02)
df<-data.frame(x=x,y=y,z=z)
ggplot(data=df,mapping=aes(x=factor(x),y=z,fill=y))+
  geom_bar(stat='identity',width=0.4,position='stack')+
  geom_text(mapping=aes(label=z),size=4,vjust=3.5,hjust=0.5,position=position_stack())+
  theme(legend.position="top")+xlab('Drive Category')+
  ylab('Response percentage(%)')+
  ggtitle('Response percentage comparison among Drive License groups')
```

From the plots above, we find the distributions of interests in insurance among gender groups are similar, however, when detecting the relationship of Drive License and response, it shows significant differences among groups. In that way, we should focus on the effect this variable leads to response in the following analyses. (Intuitive judgement)


**Logistic Regression Model**
For categorical variables, we should notice the intersections between continuous variables with categorical variables, detect by plots by categories:
Age, Premium, Vintage are three quantitative variables
Gender, Drive, Insured, Vage, Damage
Delete Code(dismiss the region effect of customers), Channel(contacting agents are various hard to follow)
```{r}
#control the categorical variable first
attach(train)
library(ggplot2)
train$y<-as.factor(train$y)
#Gender
ggplot(data=train,aes(x=y,y=Age))+geom_boxplot(notch=TRUE,aes(fill=factor(Gender)))+
  ggtitle("Response boxplot of Gender groups among Age")
ggplot(data=train)+geom_boxplot(notch=TRUE,aes(x=y,y=log(Premium),fill=factor(Gender)))+
  ggtitle("Response boxplot of Gender groups among logPremium")
ggplot(data=train)+geom_boxplot(notch=TRUE,aes(x=y,y=Vintage,fill=factor(Gender)))+
  ggtitle("Response boxplot of Gender groups among Vintage")
#Drive
ggplot(data=train,aes(x=y,y=Age))+geom_boxplot(notch=FALSE,aes(fill=factor(Drive)))+
  ggtitle("Response boxplot of Drive groups among Age")
ggplot(data=train)+geom_boxplot(notch=FALSE,aes(x=y,y=log(Premium),fill=factor(Drive)))+
  ggtitle("Response boxplot of Drive groups among logPremium")
ggplot(data=train)+geom_boxplot(notch=FALSE,aes(x=y,y=Vintage,fill=factor(Drive)))+
  ggtitle("Response boxplot of Drive groups among Vintage")
#Insured
ggplot(data=train,aes(x=y,y=Age))+geom_boxplot(notch=FALSE,aes(fill=factor(Insured)))+
  ggtitle("Response boxplot of Insured groups among Age")
ggplot(data=train)+geom_boxplot(notch=FALSE,aes(x=y,y=log(Premium),fill=factor(Insured)))+
  ggtitle("Response boxplot of Insured groups among logPremium")
ggplot(data=train)+geom_boxplot(notch=FALSE,aes(x=y,y=Vintage,fill=factor(Insured)))+
  ggtitle("Response boxplot of Insured groups among Vintage")
#Vage
ggplot(data=train,aes(x=y,y=Age))+geom_boxplot(notch=FALSE,aes(fill=factor(Vage)))+
  ggtitle("Response boxplot of Vehicle age groups among Age")
ggplot(data=train)+geom_boxplot(notch=FALSE,aes(x=y,y=log(Premium),fill=factor(Vage)))+
  ggtitle("Response boxplot of Vehicle age groups among logPremium")
ggplot(data=train)+geom_boxplot(notch=FALSE,aes(x=y,y=Vintage,fill=factor(Vage)))+
  ggtitle("Response boxplot of Vehicle age groups among Vintage")
#Damage
ggplot(data=train,aes(x=y,y=Age))+geom_boxplot(notch=TRUE,aes(fill=factor(Damage)))+
  ggtitle("Response boxplot of Damage groups among Age")
ggplot(data=train)+geom_boxplot(notch=TRUE,aes(x=y,y=log(Premium),fill=factor(Damage)))+
  ggtitle("Response boxplot of Damage groups among logPremium")
ggplot(data=train)+geom_boxplot(notch=TRUE,aes(x=y,y=Vintage,fill=factor(Damage)))+
  ggtitle("Response boxplot of Damage groups among Vintage")
```

After performing the intersection detection, we should add seven intersection variables into the model fitting: Drive*Age, Insured*Age, Vage*Age, Damage*logPremium

```{r}
#Fit a logistic regression model
train$Gender<-factor(train$Gender);train$Drive<-factor(train$Drive);train$Vage<-factor(train$Vage)
train$Damage<-factor(train$Damage);train$Insured<-factor(train$Insured)
mod1<-glm(y~Gender+Drive+Vage+Damage+Insured+Age+logPremium+Vintage+Drive*Age+
            Insured*Age+Vage*Age+Damage*logPremium,family=binomial(link='logit'),data=train)
summary(mod1)
anova(mod1,test="Chisq")

library("MASS")
library("car")
extractAIC(mod1)
extractAIC(mod1,k=log(length(train$y)))
```

From the model, we have significant variables includes Gender, Drive, Vage, Damage, Insured, Age, logPremium, Vage*Age, Damage*logPremium

```{r}
mod2<-glm(y~Gender+Drive+Vage+Damage+Insured+Age+logPremium+Drive*Age+Insured*Age+
            Vage*Age+Damage*logPremium,family=binomial(link='logit'),data=train)
summary(mod2)
extractAIC(mod2,k=log(length(train$y)))
#Test for the adequate of reduce model(mod2)
G2<-mod2$deviance-mod1$deviance
dfs<-mod2$df.residual-mod1$df.residual
qchisq(0.95,dfs)
G2
```

G2<Qchisq indicate that the reduced fitted logistic model is correct.

Build a classification table basing on the cutoff=0.5, basing on this cutoff, we have
Estimated $P(\hat{Y}=0 \mid Y=1))=244/4000=0.061$, Estimated $P(\hat{Y}=1 \mid Y=0))=2501/4000$, since the estimated errors in two groups are different significantly, we should choose another appropriate cutoff for train group. 

```{r}
library("ROCR")
library(pROC)
tab<-table(train$y,mod2$fitted.values>0.5)
addmargins(tab)
```

```{r}
#fit the estimated model to the test set
#Prediction of test data
test$Gender<-factor(test$Gender);test$Drive<-factor(test$Drive);test$Vage<-factor(test$Vage)
test$Damage<-factor(test$Damage);test$Insured<-factor(test$Insured)
attach(test)
newpred<-predict(mod1,test,type="response")

#ROC Curve for train fitted model
par(mfrow=c(1,1))
roc1<-roc(train$y,fitted(mod1))
roc2<-roc(train$y,fitted(mod2))
roc.test(roc1,roc2)
pred<-prediction(fitted(mod2),train$y)
perf<-performance(pred,"tpr","fpr")
plot(perf,main="ROC Curve for Model 1(AUC=0.8428)")
abline(0,1)
performance(pred,"auc")

#For test group 
roc3<-roc(test$y,newpred)
pred1<-prediction(newpred,test$y)
perf1<-performance(pred1,"tpr","fpr")
plot(perf1,main="ROC Curve for Model 1(AUC=0.8428)")
abline(0,1)
performance(pred1,"auc")
```


**Random Forest Prediction**

**Balanced Random Forest(down-sampling method)**

(http://appliedpredictivemodeling.com/blog/2013/12/8/28rmc2lv96h8fw8700zm4nl50busep)
(https://shiring.github.io/machine_learning/2017/04/02/unbalanced)

The area under the ROC curve will be used to quantify the effectiveness of each procedure for these data.

```{r}
setwd("/Users/zhouwenxiao/Desktop/Project of Math5637")
data<-read.csv("train.csv")
names(data)<-c("ID","Gender","Age","Drive","Code","Insured","Vage","Damage","Premium","Channel","Vintage","y")
data$logPremium<-log(data$Premium)
data$Gender<-ifelse(data$Gender=="Male",1,0)
data$Vage<-ifelse(data$Vage=="< 1 Year",1,ifelse(data$Vage=="1-2 Year",2,3))
data$Damage<-ifelse(data$Damage=="Yes",1,0)
library(caret)
library("ROCR")
library(pROC)
data$y<-factor(data$y)
set.seed(1123)
split<-createDataPartition(data$y,time=1,p=0.8,list=FALSE)
trainspl<-data[split,]
testspl<-data[-split,]
nmin<-length(trainspl$y[which(trainspl$y==1)])
trainspl$y<-ifelse(trainspl$y==0,'Class0','Class1')

#caret makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our trainControl and choose down for under- (also called down-) sampling
#strata: Tell randomForest to sample by strata; sampsize: specify that the number of samples selected within each class should be the same
ctrl<-trainControl(method="cv",classProbs=TRUE,summaryFunction=twoClassSummary)
modrf<-train(y~.,data=trainspl,method="rf",ntree=1000,tuneLenth=5,metric="ROC",trControl=ctrl,strata=trainspl$y,sampsize=rep(nmin,2))

#Prediction
downProbs<-predict(modrf,testspl, type = "prob")[,1]
testspl$y<-ifelse(testspl$y==1,2,1)
downsampledROC<-roc(testspl$y,downProbs,levels=c(1,2),direction="<")

#Determine the area under the curve
plot(downsampledROC,col=rgb(1,0,0,0.5),lwd=2,main="ROC Curve for Random Forest(AUC=0.8511)")
auc(downsampledROC)
```


**Data Clustering**

Basing on the results in Logistic regression model, we select several significant predictors as indicators when consider get data clusters. Since the sample size is so large that we only choose a small random sample to determine the clusters.

```{r}
setwd("/Users/zhouwenxiao/Desktop/Project of Math5637")
data<-read.csv("train.csv")
names(data)<-c("ID","Gender","Age","Drive","Code","Insured","Vage","Damage","Premium","Channel","Vintage","y")
data$Drive<-ifelse(data$Drive==1,1,0)
data$logPremium<-log(data$Premium)
data$Gender<-ifelse(data$Gender=="Male",1,0)
data$Vage<-ifelse(data$Vage=="< 1 Year",1,ifelse(data$Vage=="1-2 Year",2,3))
data$Damage<-ifelse(data$Damage=="Yes",1,0)
set.seed(3)
res1<-data[which(data$y==1),]
res0<-data[which(data$y==0),]
l1<-length(res1$y)
l0<-length(res0$y)
resnum<-sample(1:l1,3000,replace=FALSE)
res1new<-res1[resnum,];res0new<-res0[resnum,]
clus<-rbind(res1new,res0new)
#clus<-data.frame(clus$ID,clus$Gender,clus$Drive,clus$Vage,clus$Damage,clus$Insured,clus$Age,clus$logPremium,clus$y)
#names(clus)<-c("ID","Gender","Drive","Vage","Damage","Insured","Age","Premium","y")
newclus<-data.frame(clus$ID,clus$Gender,clus$Vage,clus$Damage,clus$Insured,clus$Age,clus$logPremium)
names(newclus)<-c("ID","Gender","Vage","Damage","Insured","Age","Premium")
```

Find the best number of clusters:
```{r}
library(factoextra)
#scale the data first
df<-scale(newclus)
fviz_nbclust(df,FUNcluster=kmeans,method="wss")+geom_vline(xintercept=3,linetype=2)
```

As we can see the most adequate category number is setting as 3 basing on the decreased slope of Sum of Square. 
```{r}
km_result<-kmeans(newclus,3,nstart=20)
#The class tag is extracted and merged with the original data
dd<-as.data.frame(cbind(newclus,cluster=km_result$cluster,clus$y))
head(dd)

cat1<-dd[dd$cluster==1,];cat2<-dd[dd$cluster==2,];cat3<-dd[dd$cluster==3,]
c1<-table(cat1$`clus$y`)
c1
c2<-table(cat2$`clus$y`)
c2
c3<-table(cat3$`clus$y`)
c3

```

Visualization:
```{r}
#visualization
library(ggplot2)
library(ggfortify)
library(cluster)
autoplot(pam(newclus,3),frame = TRUE,frame.type='norm')+labs(title = "Cluster visualization")
```

Determine the characteristics:
```{r}
#combine cluster 1 and 2 as one group, define as cluster 1
dd$cluster<-ifelse(dd$cluster==3,2,1)
library(ggplot2)
dd$cluster<-factor(dd$cluster)
ggplot(dd,aes(x=Age,fill=cluster))+geom_histogram(position="identity",alpha=0.4)
  +labs(title="Overlapped histogram of Age in two cluster")
#density plot can show the relatively density of age 
ggplot(dd,aes(x=Age,fill=cluster))+geom_density(alpha=0.5)+
  labs(title="Overlapped density plot of Age in two cluster")

#no significant difference for Logpremium, not shown
ggplot(dd,aes(x=Premium,fill=cluster))+geom_density(alpha=0.5)

#for categorical variables
d1<-dd[which(dd$cluster==1),]
d2<-dd[which(dd$cluster==2),]

G1<-table(d1$Gender)
V1<-table(d1$Vage)
D1<-table(d1$Damage)
I1<-table(d1$Insured)
prop.table(G1)
prop.table(V1)
prop.table(D1)
prop.table(I1)

G2<-table(d2$Gender)
V2<-table(d2$Vage)
D2<-table(d2$Damage)
I2<-table(d2$Insured)
prop.table(G2)
prop.table(V2)
prop.table(D2)
prop.table(I2)
```

